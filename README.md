hello world!

I am Vasudev Gupta.

I work on pre-training models at UnboxAI and lead the AI & data team. My daily job is to train & scale our [BehaviorGPT](https://unboxai.com/) model on hundreds of H100s.

I am also Google Developer Expert (GDE) in JAX.

I graduated from IIT Madras with a Dual Degree (M.Tech. Data Science + B.Tech. Mechanical Engineering).

I was mentor at Google Summer of Code 2022 @TensorFlow for the project titled: "Developing NLP examples using HuggingFace Transformers".

I participated in Google Summer of Code 2021 and build a package for training the wav2vec2 model in TensorFlow. My work got featured in [TensorFlow blog post](https://blog.tensorflow.org/2021/09/tensorflow-hubs-experience-with-gsoc-2021.html). You can read the final report of my work [here](https://thevasudevgupta.github.io/gsoc-wav2vec2/assets/final_report).

I contributed BigBird (RoBERTA & Pegasus) to HuggingFace Transformers in PyTorch & JAX. I got featured in [HuggingFace's newsletter](https://huggingface.curated.co/issues/9#start) for my contributions to Transformers.

[GitHub](https://github.com/thevasudevgupta) | [LinkedIn](https://www.linkedin.com/in/thevasudevgupta) | [Twitter](https://twitter.com/thevasudevgupta)

Blogs:
* [Optimizing adapters for NMT](https://medium.com/offnote-labs/build-a-model-which-can-translate-multiple-indian-languages-to-english-very-efficiently-reduce-55375fb0e1ea)
* [Understanding BigBird's Block Sparse Attention](https://huggingface.co/blog/big-bird)

Talks:
* [BigBird: Transformers on long sequences](https://www.youtube.com/watch?v=G22vNvHmHQ0)
* [Deep Dive into Pre-trained Transformers](https://drive.google.com/file/d/1zUq_decKNqaDkdW6_xZ0lDubo3GNEyij)

Open Source Contributions to TensorFlow:
* [Exported fine-tuned Wav2Vec2 model to TFHub](https://github.com/tensorflow/tfhub.dev/pull/68)
* [Exported pre-trained Wav2Vec2 model to TFHub](https://github.com/tensorflow/tfhub.dev/pull/65)
* [Added notebook for demonstrating Wav2Vec2 fine-tuning to TFHub](https://github.com/tensorflow/hub/pull/788)
* [Implemented & trained Wav2Vec2 model in TensorFlow](https://github.com/thevasudevgupta/gsoc-wav2vec2/commits?author=thevasudevgupta)

Open Source Contributions to HuggingFace:
* [Added PyTorch `BigBird-Pegasus` to Transformers](https://github.com/huggingface/transformers/pull/10991)
* [Added PyTorch `BigBird-RoBERTa` to Transformers](https://github.com/huggingface/transformers/pull/10183)
* [Added Flax/Jax `BigBird-RoBERTa` to Transformers](https://github.com/huggingface/transformers/pull/11967)
* [Added script for training `FlaxBigBird` on natural-questions to Transformers](https://github.com/huggingface/transformers/pull/12233)
* [Integrated `Microsoft's DeepSpeed` with Accelerate](https://github.com/huggingface/accelerate/pull/82)
* [Added `ModelHubMixin` in Hub](https://github.com/huggingface/huggingface_hub/pull/11)

Tutorials:
* [`Flax BigBird` evaluation on natural-questions dataset](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-flax-natural-questions.ipynb)
* [`PyTorch BigBird` evaluation on natural-questions dataset](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-torch-natural-questions.ipynb)
* [`PyTorch BigBirdPegasus` evaluation on PubMed dataset](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
* [How to use BigBird (RoBERTa & Pegasus) for inference](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird-inference.ipynb)
* [Template for fine-tuning a pre-trained Wav2Vec2 SavedModel](https://www.tensorflow.org/hub/tutorials/wav2vec2_saved_model_finetuning)
* [Conversion of TF Wav2Vec2 model to ONNX and compares the latency of ONNX exported model & TF model on CPU](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2_onnx.ipynb)
* [Wav2Vec2 evaluation (without any padding) on LibriSpeech data](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/librispeech_evaluation_WER_3.ipynb)
* [Wav2Vec2 SavedModel evaluation (with constant padding upto 246000 length) on LibriSpeech data](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/librispeech_evaluation_WER_6.ipynb)
* [Small demo of how to use Wav2Vec2 for inference for ASR task](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2-inference.ipynb)
